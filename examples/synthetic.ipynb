{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPE/OPL Experiments with Synthetic Bandit Data\n",
    "---\n",
    "This notebook provides an example of conducting OPE of several different evaluation policies with synthetic bandit feedback data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import (\n",
    "    SyntheticBanditDataset,\n",
    "    logistic_reward_function,\n",
    "    linear_behavior_policy,\n",
    ")\n",
    "from obp.policy import IPWLearner\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting as IPS,\n",
    "    DirectMethod as DM,\n",
    "    DoublyRobust as DR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.0\n"
     ]
    }
   ],
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Generate Synthetic Data\n",
    "\n",
    "`SyntheticBanditDataset` is an easy-to-use synthetic data generator class in the dataset module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = SyntheticBanditDataset(\n",
    "    n_actions=10, # number of actions; |A|\n",
    "    dim_context=5, # number of dimensions of context vector\n",
    "    reward_function=logistic_reward_function, # mean reward function; q(x,a)\n",
    "    behavior_policy_function=linear_behavior_policy, # behavior policy; \\pi_b\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bandit_data = dataset.obtain_batch_bandit_feedback(n_rounds=10000)\n",
    "test_bandit_data = dataset.obtain_batch_bandit_feedback(n_rounds=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_rounds': 10000,\n",
       " 'n_actions': 10,\n",
       " 'context': array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],\n",
       "        [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],\n",
       "        [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],\n",
       "        ...,\n",
       "        [-1.27028221,  0.80914602, -0.45084222,  0.47179511,  1.89401115],\n",
       "        [-0.68890924,  0.08857502, -0.56359347, -0.41135069,  0.65157486],\n",
       "        [ 0.51204121,  0.65384817, -1.98849253, -2.14429131, -0.34186901]]),\n",
       " 'action_context': array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),\n",
       " 'action': array([9, 2, 1, ..., 0, 2, 6]),\n",
       " 'position': None,\n",
       " 'reward': array([1, 1, 1, ..., 0, 1, 1]),\n",
       " 'expected_reward': array([[0.80210203, 0.73828559, 0.83199558, ..., 0.81190503, 0.70617705,\n",
       "         0.68985306],\n",
       "        [0.94119582, 0.93473317, 0.91345213, ..., 0.94140688, 0.93152449,\n",
       "         0.90132868],\n",
       "        [0.87248862, 0.67974991, 0.66965669, ..., 0.79229752, 0.82712978,\n",
       "         0.74923536],\n",
       "        ...,\n",
       "        [0.66717573, 0.81583571, 0.77012708, ..., 0.87757008, 0.57652468,\n",
       "         0.80629132],\n",
       "        [0.52526986, 0.39952563, 0.61892038, ..., 0.53610389, 0.49392728,\n",
       "         0.58408936],\n",
       "        [0.55375831, 0.11662199, 0.807396  , ..., 0.22532856, 0.42629292,\n",
       "         0.24120499]]),\n",
       " 'pscore': array([0.07250876, 0.10335615, 0.14110696, ..., 0.09756788, 0.10335615,\n",
       "        0.14065505])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_bandit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Train Bandit Policies (OPL)\n",
    "`obp.policy.IPWLearner` can be a first choice (IPS=IPW)\n",
    "\n",
    "$ \\hat{\\pi} \\in \\underset{\\pi \\in \\Pi}{\\operatorname{argmax}} \\hat{V}_{I P S}\\left(\\pi ; \\mathcal{D}_{t r}\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipw_learner = IPWLearner(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    base_classifier=LogisticRegression(C=100, random_state=12345) # any sklearn classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "ipw_learner.fit(\n",
    "    context=training_bandit_data[\"context\"], # context; x\n",
    "    action=training_bandit_data[\"action\"], # action; a\n",
    "    reward=training_bandit_data[\"reward\"], # reward; r\n",
    "    pscore=training_bandit_data[\"pscore\"], # propensity score; pi_b(a|x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict (action dist = action distribution)\n",
    "action_dist_ipw = ipw_learner.predict(\n",
    "    context=test_bandit_data[\"context\"], # context in the test data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dist_ipw[:, :, 0] # which action to take for each context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (3) Approximate the Ground-truth Policy Value\n",
    "$V(\\pi) \\approx \\frac{1}{|\\mathcal{D}_{te}|} \\sum_{i=1}^{|\\mathcal{D}_{te}|} \\mathbb{E}_{a \\sim \\pi(a|x_i)} [q(x_i, a)], \\; \\, where \\; \\, q(x,a) := \\mathbb{E}_{r \\sim p(r|x,a)} [r]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_value_of_ipw = dataset.calc_ground_truth_policy_value(\n",
    "    expected_reward=test_bandit_data[\"expected_reward\"], # expected rewards; q(x,a)\n",
    "    action_dist=action_dist_ipw, # action distribution of IPWLearner\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7544002774485176"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground-truth policy value of `IPWLearner`\n",
    "policy_value_of_ipw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (4) Off-Policy Evaluation (OPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (4-1) obtain a reward estimator\n",
    "`obp.ope.RegressionModel` simplifies the process of reward modeling\n",
    "\n",
    "$q(x,a) = \\mathbb{E} [r \\mid x, a] \\approx \\hat{q}(x,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = RegressionModel(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    base_model=LogisticRegression(C=100, random_state=12345) # any sklearn classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_rewards = regression_model.fit_predict(\n",
    "    context=test_bandit_data[\"context\"], # context; x\n",
    "    action=test_bandit_data[\"action\"], # action; a\n",
    "    reward=test_bandit_data[\"reward\"], # reward; r\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83287191, 0.77609002, 0.86301082, ..., 0.80705541, 0.87089962,\n",
       "        0.88661944],\n",
       "       [0.24064512, 0.18060694, 0.28603087, ..., 0.21010776, 0.30020355,\n",
       "        0.33212285],\n",
       "       [0.92681158, 0.89803854, 0.94120582, ..., 0.91400784, 0.94487921,\n",
       "        0.95208655],\n",
       "       ...,\n",
       "       [0.95590514, 0.93780227, 0.96479479, ..., 0.94790501, 0.96704598,\n",
       "        0.97144249],\n",
       "       [0.9041431 , 0.8677301 , 0.92262342, ..., 0.88785354, 0.92736823,\n",
       "        0.93671186],\n",
       "       [0.19985698, 0.14801146, 0.23998121, ..., 0.1733142 , 0.25267968,\n",
       "        0.28157917]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_rewards[:, :, 0] # \\hat{q}(x,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2) OPE\n",
    "`obp.ope.OffPolicyEvaluation` simplifies the OPE process\n",
    "\n",
    "$V(\\pi_e) \\approx \\hat{V} (\\pi_e; \\mathcal{D}_b, \\theta)$ using DM, IPS, and DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ope = OffPolicyEvaluation(\n",
    "    bandit_feedback=test_bandit_data, # test data\n",
    "    ope_estimators=[\n",
    "        IPS(estimator_name=\"IPS\"), \n",
    "        DM(estimator_name=\"DM\"), \n",
    "        DR(estimator_name=\"DR\"), # used estimators\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimated_policy_value = ope.estimate_policy_values(\n",
    "    action_dist=action_dist_ipw, # \\pi_e(a|x)\n",
    "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{q}(x,a)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPS': 0.7514146726298303, 'DM': 0.6496363798172724, 'DR': 0.7515054724486296}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPE results given by the three estimators\n",
    "estimated_policy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Evaluation of OPE\n",
    "Now, let's evaluate the OPE performance (estimation accuracy) of the three estimators\n",
    "\n",
    "$SE (\\hat{V}; \\mathcal{D}_b) := \\left( V(\\pi_e) - \\hat{V} (\\pi_e; \\mathcal{D}_b, \\theta) \\right)^2$,     (squared error of $\\hat{V}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_errors = ope.evaluate_performance_of_estimators(\n",
    "    ground_truth_policy_value=policy_value_of_ipw, # V(\\pi_e)\n",
    "    action_dist=action_dist_ipw, # \\pi_e(a|x)\n",
    "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{q}(x,a)\n",
    "    metric=\"se\", # squared error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPS': 8.913836133368584e-06,\n",
       " 'DM': 0.010975474246890016,\n",
       " 'DR': 8.379895987376119e-06}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_errors # DR is the most accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate the above process several times and calculate the following MSE\n",
    "\n",
    "$MSE (\\hat{V}) := T^{-1} \\sum_{t=1}^T SE (\\hat{V}; \\mathcal{D}_b^{(t)}) $\n",
    "\n",
    "where $\\mathcal{D}_b^{(t)}$ is the synthetic data in the $t$-th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('.pyenv': pyenv)",
   "language": "python",
   "name": "python37364bitpyenvpyenv99df861ad12e4e8792812ed413ad34d2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
