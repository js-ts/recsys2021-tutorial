{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPE Experiment with Multi-class Classificatoin Data\n",
    "---\n",
    "This notebook provides an example of conducting OPE of an evaluation policy using multi-class classification dataset as logged bandit data.\n",
    "\n",
    "This example notebook contains the follwoing four major steps:\n",
    "- (1) Bandit Reduction\n",
    "- (2) Off-Policy Learning\n",
    "- (3) Off-Policy Evaluation\n",
    "- (4) Evaluation of OPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import MultiClassToBanditReduction\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting,\n",
    "    DirectMethod,\n",
    "    DoublyRobust\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (1) Bandit Reduction\n",
    "`obp.dataset.MultiClassToBanditReduction` is an easy-to-use for transforming classification data to bandit data.\n",
    "It takes \n",
    "- feature vectors (`X`)\n",
    "- class labels (`y`)\n",
    "- classifier to construct behavior policy (`base_classifier_b`) \n",
    "- paramter of behavior policy (`alpha_b`) \n",
    "\n",
    "as its inputs and generates a bandit data that can be used to evaluate the performance of decision making policies (obtained by `off-policy learning`) and OPE estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw digits data\n",
    "# `return_X_y` splits feature vectors and labels, instead of returning a Bunch object\n",
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the raw classification data into a logged bandit dataset\n",
    "# we construct a behavior policy using Logistic Regression and parameter alpha_b\n",
    "# given a pair of a feature vector and a label (x, c), create a pair of a context vector and reward (x, r)\n",
    "# where r = 1 if the output of the behavior policy is equal to c and r = 0 otherwise\n",
    "# please refer to https://zr-obp.readthedocs.io/en/latest/_autosummary/obp.dataset.multiclass.html for the details\n",
    "dataset = MultiClassToBanditReduction(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    base_classifier_b=LogisticRegression(max_iter=10000, random_state=12345),\n",
    "    alpha_b=0.8,\n",
    "    dataset_name=\"digits\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the original data into training and evaluation sets\n",
    "dataset.split_train_eval(eval_size=0.7, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_actions': 10,\n",
       " 'n_rounds': 1258,\n",
       " 'context': array([[ 0.,  0.,  0., ..., 16.,  1.,  0.],\n",
       "        [ 0.,  0.,  7., ..., 16.,  3.,  0.],\n",
       "        [ 0.,  0., 12., ...,  8.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  1., 13., ...,  8., 11.,  1.],\n",
       "        [ 0.,  0., 15., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  4., ..., 15.,  3.,  0.]]),\n",
       " 'action': array([6, 8, 5, ..., 2, 5, 9]),\n",
       " 'reward': array([1., 1., 1., ..., 1., 1., 1.]),\n",
       " 'position': None,\n",
       " 'pscore': array([0.82, 0.82, 0.82, ..., 0.82, 0.82, 0.82])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain logged bandit data generated by behavior policy\n",
    "bandit_data = dataset.obtain_batch_bandit_feedback(random_state=12345)\n",
    "\n",
    "# `bandit_data` is a dictionary storing logged bandit feedback\n",
    "bandit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Off-Policy Learning\n",
    "After generating logged bandit data, we now obtain an evaluation policy using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain action choice probabilities by an evaluation policy\n",
    "# we construct an evaluation policy using Random Forest and parameter alpha_e\n",
    "action_dist = dataset.obtain_action_dist_by_eval_policy(\n",
    "    base_classifier_e=RandomForest(random_state=12345),\n",
    "    alpha_e=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.91, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       ...,\n",
       "       [0.01, 0.01, 0.91, ..., 0.01, 0.01, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n",
       "       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.91]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which action to take for each context (a probability distribution over actions)\n",
    "action_dist[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (3) Off-Policy Evaluation (OPE)\n",
    "OPE attempts to estimate the performance of evaluation policies using their action choice probabilities.\n",
    "\n",
    "Here, we evaluate/compare the OPE performance of **InverseProbabilityWeighting (IPW)**, **DirectMethod (DM)**, and **Doubly Robust (DR)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-1) obtain a reward estimator\n",
    "$q(x,a) \\approx \\hat{q}(x,a)$ with cross-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obp.ope.RegressionModel\n",
    "regression_model = RegressionModel(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    base_model=LogisticRegression(C=100, max_iter=10000, random_state=12345), # any sklearn classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_rewards = regression_model.fit_predict(\n",
    "    context=bandit_data[\"context\"],\n",
    "    action=bandit_data[\"action\"],\n",
    "    reward=bandit_data[\"reward\"],\n",
    "    position=bandit_data[\"position\"],\n",
    "    n_folds=2, # use 2-fold cross-fitting\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90532942, 0.89067336, 0.93692144, ..., 0.86292398, 0.89504067,\n",
       "        0.89454274],\n",
       "       [0.65695587, 0.47921038, 0.58244466, ..., 0.3219239 , 0.66947205,\n",
       "        0.84474829],\n",
       "       [0.77594118, 0.62461982, 0.71610604, ..., 0.46194026, 0.78553332,\n",
       "        0.90774452],\n",
       "       ...,\n",
       "       [0.76558697, 0.61078044, 0.70404139, ..., 0.44740925, 0.77549413,\n",
       "        0.90271757],\n",
       "       [0.99910077, 0.9981303 , 0.99876584, ..., 0.99638258, 0.99914974,\n",
       "        0.99968332],\n",
       "       [0.59878879, 0.41762193, 0.52085796, ..., 0.2700673 , 0.61217438,\n",
       "        0.80917434]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_rewards[:, :, 0] # \\hat{q}(x,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-2) conduct OPE\n",
    "$V(\\pi_e) \\approx \\hat{V} (\\pi_e; \\mathcal{D}_b, \\theta)$ with DM, IPW, and DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# obp.ope.OffPolicyEvaluation\n",
    "ope = OffPolicyEvaluation(\n",
    "    bandit_feedback=bandit_data, # bandit data\n",
    "    ope_estimators=[\n",
    "        InverseProbabilityWeighting(), DirectMethod(), DoublyRobust(), # used estimators\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimated_policy_value = ope.estimate_policy_values(\n",
    "    action_dist=action_dist, # \\pi_e(a|x)\n",
    "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{q}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPE results given by the three estimators\n",
    "estimated_policy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (4) Evaluation of OPE estimators\n",
    "Our final step is **the evaluation of OPE**, which evaluates and compares the estimation accuracy of OPE estimators.\n",
    "\n",
    "With the multi-class classification data, we can calculate the ground-truth policy value of the evaluation policy. \n",
    "Therefore, we can compare the policy values estimated by OPE estimators with the ground-turth to evaluate OPE estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4-1) Approximate the Ground-truth Policy Value\n",
    "$V(\\pi) \\approx \\frac{1}{|\\mathcal{D}_{te}|} \\sum_{i=1}^{|\\mathcal{D}_{te}|} \\mathbb{E}_{a \\sim \\pi(a|x_i)} [q(x_i, a)], \\; \\, where \\; \\, q(x,a) := \\mathbb{E}_{r \\sim p(r|x,a)} [r]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8770906200317964"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the ground-truth performance of the evaluation policy\n",
    "true_policy_value = dataset.calc_ground_truth_policy_value(action_dist=action_dist)\n",
    "\n",
    "true_policy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2) Evaluation of OPE\n",
    "$SE (\\hat{V}; \\mathcal{D}_b) := \\left( V(\\pi_e) - \\hat{V} (\\pi_e; \\mathcal{D}_b, \\theta) \\right)^2$,     (squared error of $\\hat{V}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_errors = ope.evaluate_performance_of_estimators(\n",
    "    ground_truth_policy_value=true_policy_value,\n",
    "    action_dist=action_dist,\n",
    "    estimated_rewards_by_reg_model=estimated_rewards,\n",
    "    metric=\"se\", # squared error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ipw': 0.00015129505568611752,\n",
       " 'dm': 0.010817510080477629,\n",
       " 'dr': 2.467173451127295e-05}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_errors # DR is the most accurate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate the above process several times and calculate the following MSE\n",
    "\n",
    "$MSE (\\hat{V}) := T^{-1} \\sum_{t=1}^T SE (\\hat{V}; \\mathcal{D}_b^{(t)}) $\n",
    "\n",
    "where $\\mathcal{D}_b^{(t)}$ is the synthetic data in the $t$-th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
